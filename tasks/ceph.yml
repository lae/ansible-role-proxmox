# This is an Ansible version of what "pveceph install" actually does
---
- block:
    - name: Install custom Ceph systemd service
      copy:
        src: /usr/share/doc/pve-manager/examples/ceph.service
        dest: /etc/systemd/system/ceph.service
        remote_src: true
        owner: root
        group: root
        mode: preserve
      notify: 'restart ceph'

    - name: Enable Ceph
      systemd:
        name: ceph.service
        enabled: true
  when:
    - "ansible_distribution_release == 'stretch'"

- block:
    - name: Create initial Ceph config
      command: "pveceph init --network {{ pve_ceph_network }}  \
      {% if pve_ceph_cluster_network is defined  %} \
                  --cluster-network {{ pve_ceph_cluster_network }}
      {% endif %}"
      args:
        creates: /etc/ceph/ceph.conf

    - name: Create initial Ceph monitor
      command: 'pveceph mon create'
      args:
        creates: '/var/lib/ceph/mon/ceph-{{ ansible_hostname }}/'
      register: _ceph_initial_mon

    - name: Fail if initial monitor creation failed
      fail:
        msg: 'Ceph intial monitor creation failed.'
      when: _ceph_initial_mon is failed
  when: "inventory_hostname == groups[pve_ceph_mon_group][0]"

- name: Create additional Ceph monitors
  command: 'pveceph mon create'
  args:
    creates: '/var/lib/ceph/mon/ceph-{{ ansible_hostname }}/'
  when: "inventory_hostname != groups[pve_ceph_mon_group][0]"

- block:
    - name: Get existing ceph volumes
      ceph_volume:
        action: list
        data: "{{ item.device }}"
      register: _ceph_volume_data
      loop: '{{ pve_ceph_osds }}'
      tags: ceph_volume
      changed_when: false #Merely gets the list of ceph volumes so never changes anything

    - name: Initialize osd variables
      set_fact:
        _existing_ceph_volumes_tmp: []
        _existing_ceph_volumes: []
      tags: ceph_volume

    - name: Determine ceph volumes Step1
      set_fact:
        _existing_ceph_volumes_tmp: "{{ _existing_ceph_volumes_tmp + item.stdout | from_json | json_query('*[].devices[]') }}"
      with_items: "{{ _ceph_volume_data.results }}"
      tags: ceph_volume

    - name: Determine ceph volumes Step2
      set_fact:
        _existing_ceph_volumes: "{{ _existing_ceph_volumes + [{'device': item}] }}"
      with_items: "{{ _existing_ceph_volumes_tmp }}"
      tags: ceph_volume

    - name: Change osd list (remove existing osds from the list)
      set_fact:
        pve_ceph_osds_diff: "{{ pve_ceph_osds | difference(_existing_ceph_volumes) }}"
      tags: ceph_volume

    - name: Create Ceph OSDs
      command: >-
        pveceph osd create {{ item.device }}
        {% if "encrypted" in item and item["encrypted"] | bool %}--encrypted 1{% endif %}
        {% if "block.db" in item %}--journal_dev {{ item["block.db"] }}{% endif %}
      args:
        creates: '{{ item.device }}1'
      with_items: '{{ pve_ceph_osds_diff }}'
  tags: create_osd

- block:
    - name: List Ceph CRUSH rules
      command: 'ceph osd crush rule ls'
      changed_when: false
      register: _ceph_crush

    - name: Create Ceph CRUSH rules
      command: >-
        ceph osd crush rule create-replicated
        {{ item.name }} default {{ item.type | default ("host") }} {{ item.class | default("") }}
      when: item.name not in _ceph_crush.stdout_lines
      with_items: '{{ pve_ceph_crush_rules }}'

    - name: Download and decompress crushmap
      command: "{{ item }}"
      with_items:
        - ceph osd getcrushmap -o crush_map_compressed
        - crushtool -d crush_map_compressed -o crush_map_decompressed
      changed_when: false # This is just getting information for us to possibly edit, don't mislead user with 'changed'

    - name: Modify crushmap for rules that should be updated
      replace:
        path: crush_map_decompressed
        regexp: >-
          rule\s+{{ item.name }}\s+{(?:(?P<space>\s+)id\s+(?P<id>[^\s]+)|\s+type\s+(?P<r_type>[^\s]+)|\s+min_size[ ](?P<min>[^\s]+)|\s+max_size\s+(?P<max>[^\s]+)|\s+step\s+take\s+default(?:\n|\s+class\s+(?P<class>[^\n]*))|\s+step\s+(?P<choose>chooseleaf|choose).*?type\s+(?P<type>[^\s]+))+(?:.|\n)*?}
        replace: >-
          rule {{item.name}} {
          \g<space>id \g<id>
          \g<space>type \g<r_type>
          \g<space>min_size {{ (pve_ceph_crush_rules | selectattr("name", "match", item.name) | list)[0].min_size | default("\g<min>") | trim }}
          \g<space>max_size {{ (pve_ceph_crush_rules | selectattr("name", "match", item.name) | list)[0].max_size | default("\g<max>") | trim }}
          {%- if ((pve_ceph_crush_rules | selectattr("name", "match", item.name) | list)[0].class | default(False)) -%}
            \g<space>step take default class {{ (pve_ceph_crush_rules | selectattr("name", "match", item.name) | list)[0].class }}
          {%- else -%}
            \g<space>step take default\g<class>
          {%- endif -%}
          \g<space>step \g<choose> firstn 0 type {{ (pve_ceph_crush_rules | selectattr("name", "match", item.name) | list)[0].type | default("\g<type>") | trim }}
          \g<space>step emit\n}
      loop: '{{ pve_ceph_crush_rules }}'
      register: _crushmap

    - name: Compress and upload changed crushmap
      command: "{{ item }}"
      with_items:
        - crushtool -c crush_map_decompressed -o new_crush_map_compressed
        - ceph osd setcrushmap -i new_crush_map_compressed
      when: _crushmap.changed

    - name: Cleanup temp files from generating new crushmap
      file:
        path:
          - crush_map_compressed
          - crush_map_decompressed
          - new_crush_map_compressed
        state: absent

    - name: List Ceph Pools
      command: ceph osd pool ls
      changed_when: false
      register: _ceph_pools

    - name: Create Ceph Pools
      command: >-
        pveceph pool create {{ item.name }}
        {% if 'storage' in item %}
        --add_storages {{ item.storage }}
        {% endif %}
        {% if 'application' in item %}
        --application {{ item.application }}
        {% endif %}
        {% if 'rule' in item %}
        --crush_rule {{ item.rule }}
        {% endif %}
        {% if 'pgs' in item %}
        --pg_num {{ item.pgs }}
        {% endif %}
        {% if 'size' in item %}
        --size {{ item.size }}
        {% endif %}
        {% if 'min_size' in item %}
        --min_size {{ item.min_size }}
        {% endif %}
      when: item.name not in _ceph_pools.stdout_lines
      with_items: '{{ pve_ceph_pools }}'
  when: "inventory_hostname == groups[pve_ceph_mon_group][0]"

- name: Create Ceph MDS servers
  command: pveceph mds create
  args:
    creates: '/var/lib/ceph/mds/ceph-{{ ansible_hostname }}'
  register: _ceph_mds_create
  when: "inventory_hostname in groups[pve_ceph_mds_group] and pve_ceph_fs"

- name: Wait for standby MDS
  command: ceph mds stat -f json
  register: _ceph_mds_stat
  until: '(_ceph_mds_stat.stdout | from_json).fsmap.standbys | length > 0'
  retries: 10
  delay: 2
  when: "_ceph_mds_create is changed"

- block:
    - name: List Ceph Filesystems
      command: ceph fs ls -f json
      changed_when: false
      when: "pve_ceph_fs | length > 0"
      register: _ceph_fs

    - name: Create Ceph Filesystems
      command: >-
        pveceph fs create
        --name {{ item.name }}
        --add-storage {{ item.storage }}
        --pg_num {{ item.pgs }}
      register: _ceph_fs_create
      failed_when: _ceph_fs_create.stderr
      when: "item.name not in (_ceph_fs.stdout | from_json | map(attribute='name'))"
      with_items: '{{ pve_ceph_fs }}'

    - name: Get Ceph Filesystem pool CRUSH rules
      command: 'ceph -f json osd pool get {{ item.0.name }}_{{ item.1 }} crush_rule'
      changed_when: false
      when: "pve_ceph_fs | length > 0"
      register: _ceph_fs_rule
      loop: '{{ pve_ceph_fs | product(["data", "metadata"]) | list }}'

    - name: Set Ceph Filesystem pool CRUSH rules
      command: >-
        ceph osd pool set {{ item.item.0.name }}_{{ item.item.1 }} crush_rule {{ item.item.0.rule }}
      when: "item.item.0.rule != (item.stdout | from_json).crush_rule"
      loop: '{{ _ceph_fs_rule.results }}'
      loop_control:
        label: '{{ item.item.0.name }}_{{ item.item.1 }}'

    - name: Create Ceph filesystem key
      command: 'ceph auth get-or-create client.{{ item.name }} osd "allow rw pool={{ item.name }}_data" mon "allow r" mds "allow rw"'
      register: _ceph_fs_auth
      changed_when: '"added key" in _ceph_fs_auth.stdout'
      when: "item.mountpoint is defined"
      loop: '{{ pve_ceph_fs }}'
  when: inventory_hostname == groups[pve_ceph_mon_group][0]

- name: Fetch Ceph filesystem key
  command: 'ceph auth print-key client.{{ item.name }}'
  args:
    creates: '/etc/ceph/{{ item.name }}.secret'
  register: _ceph_fs_key
  when: "item.mountpoint is defined"
  loop: '{{ pve_ceph_fs }}'

- name: Save Ceph filesystem key
  copy:
    dest: '/etc/ceph/{{ item.item.name }}.secret'
    owner: 'root'
    group: 'root'
    mode: '0600'
    content: '{{ item.stdout }}'
  when: "item is changed"
  loop: '{{ _ceph_fs_key.results }}'
  loop_control:
    label: '{{ item.item }}'

- name: Mount Ceph filesystems
  mount:
    path: '{{ item.mountpoint }}'
    src: |-
      {% for h in groups[pve_ceph_mon_group] -%}
      {{ hostvars[h].ansible_all_ipv4_addresses | ipaddr(pve_ceph_network) | first -}}
      {{ loop.last | ternary("", ",") -}}
      {% endfor %}:/
    fstype: 'ceph'
    opts: 'name={{ item.name }},secretfile=/etc/ceph/{{ item.name }}.secret,_netdev'
    state: 'mounted'
  when: "item.mountpoint is defined"
  loop: '{{ pve_ceph_fs }}'
